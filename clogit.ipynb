{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"provenance":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u7VjEOmIUx_A","executionInfo":{"status":"ok","timestamp":1628091429190,"user_tz":240,"elapsed":28692,"user":{"displayName":"Ramin Forouzandeh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitixDhaXGnkxs4MhLe_Vi6FQ7XGtruxlpI4nT3Fg=s64","userId":"08807228560659022036"}},"outputId":"71a70495-742c-4ac9-8c80-c7e8facfb243"},"source":["from google.colab import drive\n","drive.mount('/content/drive/', force_remount=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"87F7bjOOU3vY","executionInfo":{"status":"ok","timestamp":1628091442424,"user_tz":240,"elapsed":13252,"user":{"displayName":"Ramin Forouzandeh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitixDhaXGnkxs4MhLe_Vi6FQ7XGtruxlpI4nT3Fg=s64","userId":"08807228560659022036"}},"outputId":"7cce0b1a-59d8-4004-8e86-250586be0b76"},"source":["%cd drive/MyDrive/chunks\n","\n","import pandas as pd\n","import numpy as np\n","import torch\n","import matplotlib.pyplot as plt\n","import random\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.optim import Adam, Adagrad\n","from torch.utils.data import DataLoader, Dataset\n","from torch.autograd.functional import hessian,jacobian\n","#torch.autograd.set_detect_anomaly(True)\n","import time\n","\n","\n","use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n","torch.backends.cudnn.benchmark = True\n","\n","class MigrationPredictor(nn.Module):\n","    def __init__(self, num_feature, num_classes):\n","        super(MigrationPredictor, self).__init__()\n","        self.layer_1 = nn.Linear(num_feature, num_classes)\n"," \n","    def forward(self,data):\n","      out = self.layer_1(data)\n","      return out\n"," \n","    def get_probs(self,data):\n","      #with torch.no_grad():\n","      return self.forward(data)\n","\n","#log-likelihood loss with weights\n","def loss(probs,target,IDs,sample_weights):\n","    logf = torch.empty(IDs.size()).to(device) \n","    for ID in torch.unique(IDs):\n","        logf[IDs==ID] = torch.logsumexp(probs[IDs==ID],0) \n","    return -torch.mean( (probs*target - logf/741) *sample_weights )"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/chunks\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iJF6X0CeVsyD"},"source":["#accumulate batches to make larger 5x batches and save as parquet to run faster\n","# this needs to be run only once to create datasets\n","for batch in range(0,2155,5):\n","  print(batch)\n","  new_batch = int(batch/5)\n","  if batch<2155:\n","    df = pd.concat([pd.read_stata(f\"data_chunk{batch+i}.dta\") for i in range(5)])\n","  else:\n","    df = pd.concat([pd.read_stata(f\"data_chunk{batch+i}.dta\") for i in range(1)])\n","  df['m'] = df['czone_orig']!=df['czone']\n","  df['wage_hat'] = (df['wage_hat']) /10000\n","  df['spouse_wage_hat'] = (df['spouse_wage_hat']) /10000\n","  df['distance_2'] = (df['distance']**2)/10000\n","  df['distance'] = df['distance']/100\n","  df.to_parquet(f'../chunks_pickle/df{new_batch}.parquet')\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D94EZYFUgWfQ"},"source":["unique_cz = 741\n","try:\n","  model = MigrationPredictor(num_feature=745, num_classes=1) \n","  optimizer = Adam(model.parameters(), lr=0.02)\n","  checkpoint = torch.load('checkpoint.pth')\n","  epoch = 1 #checkpoint['epoch']\n","  batch = checkpoint['batch']\n","  loss_trend = checkpoint['loss_trend']\n","  total_loss_trend = checkpoint['total_loss_trend']\n","  mean_loss_old = checkpoint['mean_loss_old']\n","  beta_m_vector = checkpoint['beta_m_vector']\n","  beta_f_vector = checkpoint['beta_f_vector']  \n","  beta_mig_vector = checkpoint['beta_mig_vector']  \n","  beta_d_vector = checkpoint['beta_d_vector']  \n","  beta_d2_vector = checkpoint['beta_d2_vector']  \n","  beta_m_trend = checkpoint['beta_m_trend']\n","  beta_f_trend = checkpoint['beta_f_trend']  \n","  beta_mig_trend = checkpoint['beta_mig_trend']  \n","  beta_d_trend = checkpoint['beta_d_trend']  \n","  beta_d2_trend = checkpoint['beta_d2_trend']  \n","  model.load_state_dict(torch.load('checkpoint.pth')['model'])\n","  model.to(device)\n","  model.train()\n","  optimizer.load_state_dict(checkpoint['optimizer'])\n","  print(f'checkpoint loaded ')\n","except:\n","  model = MigrationPredictor(num_feature=745, num_classes=1) \n","  model.to(device)\n","\n","  optimizer = Adam(model.parameters(), lr=0.02)\n","  mean_loss_old=np.inf\n","  total_loss_trend=[]\n","  epoch = 0\n","  beta_m_trend=[]\n","  beta_f_trend=[]\n","  beta_mig_trend=[]\n","  beta_d_trend=[]  \n","  beta_d2_trend=[]\n","while True:\n","  loss_trend = []\n","  i=0\n","  beta_m_vector = []\n","  beta_f_vector = []\n","  beta_d_vector = []\n","  beta_d2_vector= []\n","  beta_mig_vector = []\n","  for batch in range(430): #np.random.permutation(430):\n","\n","      st=time.time()\n","      i=i+1\n","      #df=pd.read_stata(f\"/content/drive/MyDrive/chunks/data_chunk{batch}.dta\")\n","      #df=pd.read_stata(f\"../chunks_pickle/df{batch}.dta\")\n","      #df=pd.read_pickle(f\"../chunks_pickle/df{batch}.pkl\")\n","      df = pd.read_parquet(f'../chunks_pickle/df{batch}.parquet') \n","      #print(len(df)/741)\n","      #dimention=N*CZ*M, M=number of variables, N=number of people\n","      X_train = torch.tensor(df[['wage_hat','spouse_wage_hat']].values.reshape(-1,unique_cz,2)).float().to(device)       \n","      y_train = torch.tensor(df['migrated'].values.reshape(-1,unique_cz,1)).bool().to(device)\n","      sample_weights = torch.tensor(df['perwt'].values.reshape((-1,unique_cz,1))).int().to(device)\n","      czone_fe = torch.tensor(pd.get_dummies(df['czone']).values.reshape(-1,unique_cz,unique_cz)[:,:,0:-1]).bool().to(device)\n","      m = torch.tensor(df['m'].values.reshape(-1,unique_cz,1)).bool().to(device)\n","      distance = torch.tensor(df['distance'].values.reshape(-1,unique_cz,1)).int().to(device)\n","      distance_2 = torch.tensor(df['distance_2'].values.reshape(-1,unique_cz,1)).int().to(device)\n","      data_input = torch.cat((X_train, m, distance, distance_2, czone_fe), 2).to(device) #TAKE THIS BACK WHEN INCLUDING FEs\n","      \n","      IDs = torch.tensor(df['serial'].values.reshape(-1,unique_cz,1).astype(int)).to(device)\n","      optimizer.zero_grad()\n","      probs = model.get_probs(data_input).to(device)\n","      out = loss(probs, y_train, IDs, sample_weights).to(device)\n","      \n","      out.backward()\n","      optimizer.step()\n","\n","      print(f\" beta_m: {list(model.parameters())[0][0][0].item()} ,  beta_f: {list(model.parameters())[0][0][1].item()} ,  beta_d: {list(model.parameters())[0][0][3].item()} , beta_d2: {list(model.parameters())[0][0][4].item()} , loss: {out} , epoch: {epoch}   , batch: {batch}  , time: {time.time()-st} , mean_loss: {mean_loss_old} \")\n","      loss_trend.append(out.cpu().detach().numpy())\n","      \n","      beta_m_vector.append(list(model.parameters())[0][0][0].item())\n","      beta_f_vector.append(list(model.parameters())[0][0][1].item())\n","      beta_mig_vector.append(list(model.parameters())[0][0][2].item())\n","      beta_d_vector.append(list(model.parameters())[0][0][3].item())\n","      beta_d2_vector.append(list(model.parameters())[0][0][4].item())\n","      \n","      #clear cuda cash to avoid CUDA memory runtime error resulting from allocation of memory to variable tensor sizes (didn't need it for when tensor sizes are the same)\n","      del [data_input,X_train,y_train,m,distance,distance_2,czone_fe,sample_weights]\n","      torch.cuda.empty_cache() \n","\n","  #save checkpoint in case the code is interrupted      \n","  checkpoint = { \n","  'batch': batch,\n","  'model': model.state_dict(),\n","  'optimizer': optimizer.state_dict(),\n","  'beta_m_vector': beta_m_vector,\n","  'beta_f_vector': beta_f_vector,\n","  'beta_mig_vector': beta_mig_vector,\n","  'beta_d_vector': beta_d_vector,\n","  'beta_d2_vector': beta_d2_vector,\n","  'beta_m_trend': beta_m_trend,\n","  'beta_f_trend': beta_f_trend,\n","  'beta_mig_trend': beta_mig_trend,\n","  'beta_d_trend': beta_d_trend,\n","  'beta_d2_trend': beta_d2_trend,\n","  'loss_trend': loss_trend,\n","  'beta_mig_trend': beta_mig_trend,\n","  'beta_d_trend': beta_d_trend,\n","  'beta_d2_trend': beta_d2_trend,\n","  'mean_loss_old': mean_loss_old,\n","  'total_loss_trend': total_loss_trend,\n","  }\n","  torch.save(checkpoint, 'checkpoint.pth')\n","  total_loss_trend.append(out)\n","  mean_loss_new = np.average(loss_trend)\n","  epoch = epoch+1\n","  beta_m_trend.append(beta_m_vector)\n","  beta_f_trend.append(beta_f_vector)\n","  beta_mig_trend.append(beta_mig_vector)\n","  beta_d_trend.append(beta_d_vector)\n","  beta_d2_trend.append(beta_d2_vector)    \n","  \n","\n","  if abs(mean_loss_old-mean_loss_new)<0.001:\n","      break\n","  else:\n","      mean_loss_old=mean_loss_new\n","  pd.DataFrame({'b_m':beta_m_trend,'b_f':beta_f_trend, 'b_mig':beta_mig_trend, 'b_d':beta_d_trend, 'b_d2':beta_d2_trend}).to_csv('trends_dataframe')\n","\n","beta_m_final = np.average(beta_m_vector )\n","beta_f_final = np.average(beta_f_vector)\n","beta_d_final = np.average(beta_d_vector)\n","beta_d2_final = np.average(beta_d2_vector)\n","beta_mig_final = np.average(beta_mig_vector)\n","print(f\"final coefficients: beta_m={beta_m_final},beta_f={beta_f_final},beta_mig={beta_mig_final},beta_d={beta_d_final},beta_d2={beta_d2_final}\")\n","\n","#save trends to convergence\n","pd.DataFrame({'b_m':beta_m_trend,'b_f':beta_f_trend, 'b_mig':beta_mig_trend, 'b_d':beta_d_trend, 'b_d2':beta_d2_trend, 'epoch':epoch, 'batch':batch}).to_csv('trends_dataframe')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8BBwG3tjJOQ_","executionInfo":{"elapsed":1076,"status":"ok","timestamp":1627574543064,"user":{"displayName":"Ramin Forouzandeh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitixDhaXGnkxs4MhLe_Vi6FQ7XGtruxlpI4nT3Fg=s64","userId":"08807228560659022036"},"user_tz":240},"outputId":"2b7410a0-6d26-4347-c08e-5da693758e16"},"source":["pd.get_dummies(df['czone']).values.reshape(-1,unique_cz,unique_cz)[:,:,0:-1].shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1995, 741, 740)"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"TeZNyJmIglyW","colab":{"base_uri":"https://localhost:8080/","height":231},"executionInfo":{"status":"error","timestamp":1628037340267,"user_tz":240,"elapsed":118,"user":{"displayName":"Ramin Forouzandeh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitixDhaXGnkxs4MhLe_Vi6FQ7XGtruxlpI4nT3Fg=s64","userId":"08807228560659022036"}},"outputId":"655a09df-43f4-4338-c1ed-41dec9cce1f6"},"source":["  model = MigrationPredictor(num_feature=746, num_classes=1) \n","  optimizer = Adam(model.parameters(), lr=0.001)\n","  checkpoint = torch.load('checkpoint.pth')\n","  epoch = checkpoint['epoch']\n","  batch = checkpoint['batch']\n","  loss_trend = checkpoint['loss_trend']\n","  total_loss_trend = checkpoint['total_loss_trend']\n","  mean_loss_old = checkpoint['mean_loss_old']\n","  beta_m_vector = checkpoint['beta_m_vector']\n","  beta_f_vector = checkpoint['beta_f_vector']  \n","  beta_mig_vector = checkpoint['beta_mig_vector']  \n","  beta_d_vector = checkpoint['beta_d_vector']  \n","  beta_d2_vector = checkpoint['beta_d2_vector']  \n","  beta_m_trend = checkpoint['beta_m_trend']\n","  beta_f_trend = checkpoint['beta_f_trend']  \n","  beta_mig_trend = checkpoint['beta_mig_trend']  \n","  beta_d_trend = checkpoint['beta_d_trend']  \n","  beta_d2_trend = checkpoint['beta_d2_trend']  \n","  model.load_state_dict(torch.load('checkpoint.pth')['model'])\n","  model.to(device)\n","  model.train()\n","  optimizer.load_state_dict(checkpoint['optimizer'])\n","  print(f'checkpoint loaded at epoch {epoch}')"],"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-dff9887ffa85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'checkpoint.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mloss_trend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss_trend'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'epoch'"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":460},"id":"zwGC2MF68aR-","executionInfo":{"status":"error","timestamp":1628102482713,"user_tz":240,"elapsed":7447,"user":{"displayName":"Ramin Forouzandeh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitixDhaXGnkxs4MhLe_Vi6FQ7XGtruxlpI4nT3Fg=s64","userId":"08807228560659022036"}},"outputId":"f24063f8-cbd0-4125-9482-388b1dca8a12"},"source":["import pandas as pd\n","import numpy as np\n","import torch\n","import matplotlib.pyplot as plt\n","import random\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.optim import Adam, Adagrad\n","from torch.utils.data import DataLoader, Dataset\n","from torch.autograd.functional import hessian,jacobian\n","#torch.autograd.set_detect_anomaly(True)\n","import time\n","\n","%cd drive/MyDrive/chunks\n","\n","use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n","torch.backends.cudnn.benchmark = True\n","\n","class MigrationPredictor(nn.Module):\n","    def __init__(self, num_feature, num_classes):\n","        super(MigrationPredictor, self).__init__()\n","        self.layer_1 = nn.Linear(num_feature, num_classes)\n"," \n","    def forward(self,data):\n","      out = self.layer_1(data)\n","      return out\n"," \n","    def get_probs(self,data):\n","      #with torch.no_grad():\n","      return self.forward(data)\n","\n","\n","#log-likelihood loss with weights\n","def loss(probs,target,IDs,sample_weights):\n","    logf = torch.empty(IDs.size()).to(device) \n","    for ID in torch.unique(IDs):\n","        logf[IDs==ID] = torch.logsumexp(probs[IDs==ID],0) \n","    return -torch.mean( (probs*target - logf/741) *sample_weights )\n","\n","model = MigrationPredictor(num_feature=745, num_classes=1) \n","optimizer = Adam(model.parameters(), lr=0.01)\n","checkpoint = torch.load('checkpoint.pth')\n","#epoch = checkpoint['epoch']\n","batch = checkpoint['batch']\n","loss_trend = checkpoint['loss_trend']\n","total_loss_trend = checkpoint['total_loss_trend']\n","mean_loss_old = checkpoint['mean_loss_old']\n","beta_m_vector = checkpoint['beta_m_vector']\n","beta_f_vector = checkpoint['beta_f_vector']  \n","beta_mig_vector = checkpoint['beta_mig_vector']  \n","beta_d_vector = checkpoint['beta_d_vector']  \n","beta_d2_vector = checkpoint['beta_d2_vector']  \n","beta_m_trend = checkpoint['beta_m_trend']\n","beta_f_trend = checkpoint['beta_f_trend']  \n","beta_mig_trend = checkpoint['beta_mig_trend']  \n","beta_d_trend = checkpoint['beta_d_trend']  \n","beta_d2_trend = checkpoint['beta_d2_trend']  \n","model.load_state_dict(torch.load('checkpoint.pth')['model'])\n","model.to(device)\n","model.train()\n","optimizer.load_state_dict(checkpoint['optimizer'])\n","#print(f'checkpoint loaded at epoch {epoch}')\n","\n","def loss0(a):\n","  total_len = 0\n","  output = torch.tensor([0]).to(device)\n","  batches=range(2)\n","  unique_cz=741\n","  for batch in range(430):\n","    print(f\"batch={batch}\")\n","    df = pd.read_parquet(f'../chunks_pickle/df{batch}.parquet') \n","    total_len = total_len + len(df['serial'])\n","    X_train0 = torch.tensor(df['wage_hat'].values.reshape(-1,unique_cz,1)).float().to(device)       \n","    X_train1 = torch.tensor(df['spouse_wage_hat'].values.reshape(-1,unique_cz,1)).float().to(device)       \n","    y_train = torch.tensor(df['migrated'].values.reshape(-1,unique_cz,1)).bool().to(device)\n","    sample_weights = torch.tensor(df['perwt'].values.reshape((-1,unique_cz,1))).int().to(device)\n","    IDs = torch.tensor(df['serial'].values.reshape(-1,unique_cz,1).astype(int)).to(device)\n","\n","    probs0 = (a[0]*X_train0+a[1]*X_train1)\n","    logf0 = torch.empty(IDs.size()).to(device) \n","    for ID in torch.unique(IDs):\n","      logf0[IDs==ID] = torch.logsumexp(probs0[IDs==ID],0) \n","    output = output -torch.mean( (probs0*y_train - logf0/741)*sample_weights )\n","    print(output)\n","    \n","    #clear cuda cash to avoid CUDA memory runtime error resulting from allocation of memory to variable tensor sizes (didn't need it for when tensor sizes are the same)\n","    del [X_train0,X_train1,y_train,IDs,probs0,logf0,sample_weights]\n","    torch.cuda.empty_cache()\n","    #X_train0 = torch.empty\n","    #X_train1 = torch.empty\n","    #y_train = torch.empty\n","    #sample_weights = torch.empty\n","    #IDs = torch.empty\n","    #probs0 = torch.empty\n","    #logf0 = torch.empty\n","  return output #/427\n","\n","\n","torch.sqrt(torch.inverse(hessian(loss0,torch.tensor([list(model.parameters())[0][0][0].item(),list(model.parameters())[0][0][1].item()]))))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/chunks\n","batch=0\n","tensor([0.3544], device='cuda:0', grad_fn=<SubBackward0>)\n","batch=1\n","tensor([0.7709], device='cuda:0', grad_fn=<SubBackward0>)\n","batch=2\n"],"name":"stdout"},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-3cfe88def1d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhessian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/functional.py\u001b[0m in \u001b[0;36mhessian\u001b[0;34m(func, inputs, create_graph, strict, vectorize)\u001b[0m\n\u001b[1;32m    699\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjacobian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjac_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_tuple_postprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_inputs_tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_inputs_tuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/functional.py\u001b[0m in \u001b[0;36mjacobian\u001b[0;34m(func, inputs, create_graph, strict, vectorize)\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_grad_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneed_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m         is_outputs_tuple, outputs = _as_tuple(outputs,\n\u001b[1;32m    484\u001b[0m                                               \u001b[0;34m\"outputs of the user-provided function\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/functional.py\u001b[0m in \u001b[0;36mjac_func\u001b[0;34m(*inp)\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mjac_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m         \u001b[0mjac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjacobian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_single_output_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    698\u001b[0m         \u001b[0m_check_requires_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"jacobian\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/functional.py\u001b[0m in \u001b[0;36mjacobian\u001b[0;34m(func, inputs, create_graph, strict, vectorize)\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_grad_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneed_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m         is_outputs_tuple, outputs = _as_tuple(outputs,\n\u001b[1;32m    484\u001b[0m                                               \u001b[0;34m\"outputs of the user-provided function\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/functional.py\u001b[0m in \u001b[0;36mensure_single_output_function\u001b[0;34m(*inp)\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mensure_single_output_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 684\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    685\u001b[0m         \u001b[0mis_out_tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_as_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"outputs of the user-provided function\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hessian\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0m_check_requires_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"outputs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-1-3cfe88def1d1>\u001b[0m in \u001b[0;36mloss0\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mlogf0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIDs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mID\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIDs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m       \u001b[0mlogf0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIDs\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mID\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogsumexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIDs\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mID\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprobs0\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0my_train\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlogf0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m741\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msample_weights\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 15.78 GiB total capacity; 14.45 GiB already allocated; 16.75 MiB free; 14.46 GiB reserved in total by PyTorch)"]}]},{"cell_type":"code","metadata":{"id":"q8crOChop58J"},"source":["torch.cuda.empty_cache()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UDqlQQ60YW9x","executionInfo":{"elapsed":2213,"status":"ok","timestamp":1621225832695,"user":{"displayName":"Ramin Forouzandeh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitixDhaXGnkxs4MhLe_Vi6FQ7XGtruxlpI4nT3Fg=s64","userId":"08807228560659022036"},"user_tz":240},"outputId":"6cc8c429-d4a5-439a-ad2c-7acff70362be"},"source":["model = MigrationPredictor(num_feature=746, num_classes=1) \n","optimizer = Adam(model.parameters(), lr=0.01)\n","checkpoint = torch.load('checkpoint.pth')\n","epoch = checkpoint['epoch']\n","batch = checkpoint['batch']\n","model.load_state_dict(torch.load('checkpoint.pth')['model'])\n","model.to(device)\n","optimizer.load_state_dict(checkpoint['optimizer'])\n","\n","output=0\n","unique_cz=741\n","df = pd.read_parquet(f'../chunks_pickle/df0.parquet') \n","X_train0 = torch.tensor(df['wage_hat'].values.reshape(-1,741,1)).float().to(device)   \n","X_train1 = torch.tensor(df['spouse_wage_hat'].values.reshape(-1,unique_cz,1)).float().to(device)       \n","y_train = torch.tensor(df['migrated'].values.reshape(-1,unique_cz,1)).bool().to(device)\n","sample_weights = torch.tensor(df['perwt'].values.reshape((-1,unique_cz,1))).int().to(device)\n","\n","\n","IDs = torch.tensor(df['serial'].values.reshape(-1,unique_cz,1).astype(int)).to(device)\n","\n","optimizer.zero_grad()\n","probs0 = (list(model.parameters())[0][0][0].item()*X_train0+list(model.parameters())[0][0][1].item()*X_train1)\n","logf0 = torch.empty(IDs.size()).to(device) \n","for ID in torch.unique(IDs):\n","  logf0[IDs==ID] = torch.logsumexp(probs0[IDs==ID],0) \n","output = output -torch.mean( (probs0*y_train - logf0/741)*sample_weights )\n","output"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(0.4400, device='cuda:0')"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"CZju-KAKJjhb"},"source":["#end of main code=========================================================================================\n","#end of main code=========================================================================================\n","#end of main code=========================================================================================\n","#end of main code========================================================================================="],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QjnulbvMo7Z2"},"source":["torch.tensor([list(model.parameters())[0][0][0].item(),list(model.parameters())[0][0][1].item(),list(model.parameters())[1].item()])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GThR3JS4WOmx"},"source":["import os\n","#torch.multiprocessing.set_start_method('forkserver', force=True)\n","#torch.multiprocessing.set_start_method('spawn', force=True)# good solution !!!!\n","\n","class MyDataset(Dataset):\n","    def __init__(self):\n","        self.data_files = os.listdir()\n","        #sort(self.data_files)\n","        #self.y = torch.tensor(self.data_files['migrated'].values.reshape(unique_individuals,unique_cz,1)).float().to(device)\n","        #self.X = torch.tensor(self.data_files[['wage_hat','spouse_wage_hat']].values.reshape(unique_individuals,unique_cz,2)).float().to(device)   \n","\n","    def __getindex__(self, idx):\n","        return pd.read_stata(self.data_files[idx])\n","\n","    def __len__(self):\n","        return len(self.data_files)\n","    \n","    def __getitem__(self, idx):\n","        df = self.__getindex__(idx)\n","        df['m'] = df['czone_orig']!=df['czone']\n","        df['wage_hat'] = (df['wage_hat']) /10000\n","        df['spouse_wage_hat'] = (df['spouse_wage_hat']) /10000\n","        #dimention=N*CZ*M, M=number of variables, N=number of people\n","        X_train = torch.tensor(df[['wage_hat','spouse_wage_hat']].values.reshape(-1,unique_cz,2)).float().to(device)       \n","        df['migrated'] = df['czone_dest']==df['czone']\n","        self.y_train = torch.tensor(df['migrated'].values.reshape(-1,unique_cz,1)).bool().to(device)\n","        self.sample_weights = torch.tensor(df['perwt'].values.reshape((-1,unique_cz,1))).int().to(device)\n","        #print(time.time()-st)\n","        czone_fe = torch.tensor(pd.get_dummies(df['czone']).values.reshape(-1,unique_cz,unique_cz)).bool().to(device)\n","        #print(\"fe\",time.time()-st)\n","        m = torch.tensor(df['m'].values.reshape(-1,unique_cz,1)).bool().to(device)\n","        \n","        self.data_input = torch.cat((X_train, m, czone_fe), 2).to(device) #TAKE THIS BACK WHEN INCLUDING FEs\n","        \n","        self.IDs = torch.tensor(df['serial'].values.reshape(-1,unique_cz,1).astype(int)).to(device)\n","        return self.y_train, self.data_input, self.IDs, self.sample_weights\n","\n","\n","dset = MyDataset()\n","loader = DataLoader(dset, num_workers=0, shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BnSZ9kNWW9iq"},"source":["unique_cz = 741\n","\n","try:\n","  model = MigrationPredictor(num_feature=744, num_classes=1)#.to(device) #743, num_classes=1) CHNAGE WHEN FEs ARE INCLUDED\n","  model.to(device)\n","\n","  optimizer = Adam(model.parameters(), lr=0.2)\n","  mean_loss_old=np.inf\n","  mean_loss_trend=[]\n","\n","except:\n","  model = torch.load(\"mymodel\")\n","while True:\n","    epoch = epoch+1\n","    loss_trend = []\n","    i=0\n","    st=time.time()\n","    for c, data in enumerate(loader,0):\n","        print(c)\n","        \n","        i=i+1\n","\n","        y_train, data_input, IDs, sample_weights = data\n","        \n","        \n","        #print(\"before optimize\",time.time()-st)\n","        optimizer.zero_grad()\n","        probs = model.get_probs(data_input).to(device)\n","        out = loss(probs, y_train, IDs, sample_weights) \n","        #print(\"before backward\",time.time()-st)\n","        \n","        out.backward()\n","        #print(\"before step\",time.time()-st)\n","\n","        optimizer.step()\n","        #print(\"after optimize\",time.time()-st)\n","\n","        print(f\" beta_m: {list(model.parameters())[0][0][0].item()}, beta_f: {list(model.parameters())[0][0][1].item()}, loss: {out}, epoch: {epoch}, batch: {i}th,{batch}, time: {time.time()-st}, mean_loss: {mean_loss_old}\")\n","        #print(\"after print\",time.time()-st)\n","        loss_trend.append(out)\n","    mean_loss_new = np.sum(loss_trend)\n","    torch.save(model,\"mymodel\")\n","    if abs(mean_loss_old-mean_loss_new)<0.00001:\n","        break\n","    else:\n","        mean_loss_old=mean_loss_new\n","    mean_loss_trend.append(mean_loss_new)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jDO5bqLNDeKi"},"source":["class my_Dataset(Dataset):\n","    # Characterizes a dataset for PyTorch\n","    def __init__(self, data_paths, target_paths, transform=None):\n","        self.data_paths = data_paths\n","        self.target_paths = target_paths\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.data_paths)\n","\n","    def __getitem__(self, index):\n","        x = torch.from_numpy(np.load(self.data_paths[index]))\n","        y = torch.from_numpy(np.load(self.target_paths[index]))\n","        if self.transform:\n","            x = self.transform(x)\n","\n","        return x, y\n","dataset = my_Dataset(data_paths = os.listdir(),target_paths = os.listdir(), transform=None)\n","loader = DataLoader(\n","    dataset,\n","    batch_size=5,\n","    shuffle=True,\n","    num_workers=2\n",")"],"execution_count":null,"outputs":[]}]}